{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Latent) Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Variational Autoencoder (VAE), let's first have a look at the theory behind.\n",
    "\n",
    "**Goal**: We want to sample from the true underlying distribution of our data $p^*(x)$. \n",
    "\n",
    "**Problem**: We don't know $p^*(x)$\n",
    "\n",
    "**Solution**: Similar to the Variational Autoencoder, we aim to approximate the distribution by learning a model distribution $p_\\theta(x)$\n",
    "\n",
    "However, different to the VAE, the diffusion model is a chain of latent variables ($z_1, ..., z_N$). To get $p^*(x)$, we need to marginalize out all latent variables: $p^*(x)=\\int p_\\theta(x_0, z_{1:N})dz_1 ... dz_N$\n",
    "\n",
    "This is intractable to compute. \n",
    "\n",
    "But let's first define what the joint distribution $p_\\theta(x, z_{1:N})$ actually is. For an image, $x_0$, we may write $p_\\theta(x_0, z_{1:N})=p(z_N)\\prod_{n>1}p_\\theta(z_{n-1}|z_n)p_\\theta(x_0|z_1)$\n",
    "\n",
    "Additionally, we define the individual distributions as:\n",
    "\n",
    "- $p_\\theta(z_{n-1}|z_n)=\\mathcal{N}(\\mu_\\theta(z_n,n),\\Sigma_\\theta(z_n,n))$ --> $z_n$ is the previous latent state; n is where we are in the process\n",
    "- $p_\\theta(x_0|z_1)=\\mathcal{N}(\\mu_\\theta(z_1,1),\\Sigma_\\theta(z_1,1))$\n",
    "- $p(z_N)=\\mathcal{N}(0,\\mathcal{I})$ --> our start (complete noise)\n",
    "\n",
    "Note: Usually, $\\mu_\\theta$ is learned with a neural network, while $\\Sigma_\\theta$ is fixed instead of learned.\n",
    "\n",
    "<br />\n",
    "\n",
    "### How can we learn the neural network?\n",
    "As for the VAE, we use Variational Inference!\n",
    "\n",
    "To learn our parameters, $\\theta$, we use the ELBO method again. Namely, we try to approximate the distribution $p(z_{1:N}|x_0)$ with a distribution $q(z_{1:N})$ just to learn $\\theta$.\n",
    "\n",
    "<br />\n",
    "\n",
    "### Choosing q (Forward Process)\n",
    "We choose q to be the following factorization $q_{\\phi(x_0)}(z_1,...,z_N)=q_{\\phi(x_0)}(z_1)\\prod_{n>1}q_{\\phi(x_0)}(z_n|z_{n-1})$\n",
    "\n",
    "Every image has its own q distribution. For the VAE we used amortized inference to learn a network that predicts the optimal parameters of q for each image. However, for the Diffusion Model, we are not learning the parameters of q, instead we fix them as follows:\n",
    "- $q_{\\phi(x_0)}(z_1)=\\mathcal{N}(\\sqrt{1-\\beta_1}x_0,\\beta_1\\mathcal(I))$\n",
    "- $q_{\\phi(x_0)}(z_n|z_{n-1})=\\mathcal{N}(\\sqrt{1-\\beta_n}z_{n-1},\\beta_n\\mathcal(I))$\n",
    "- $0<\\beta_1<...<\\beta_N<1$ --> scaling the noise such that each diffusion step (n) makes the image noisier\n",
    "\n",
    "--> This (markov) process is called the **forward process** (i.e. the noising process)\n",
    "\n",
    "Remember: q is only a helper to learn $\\theta$ and thereby approximate $\\log p_\\theta(x_0)$. \n",
    "\n",
    "We make it easy for ourselves by fixing its parameters and by letting q be a Gaussian distribution because a) it simplifies later calculations and b) it makes sense to assume that $q_{\\phi(x_0)}(z_n|z_{n-1})$ is a Gaussian since $p_\\theta(z_{n-1}|z_n)$ is a Gaussian.\n",
    "\n",
    "Note: by formulating q in this way, we also impose that $p(z_{1:N}|x_0)$ approximates the noising trajectory and follows a Gaussian distribution\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Reparameterization of q\n",
    "With our parameterization of q, it is possible to sample $z_n$ (or any other $z_{n-i}$) directly from $x_0$, making training a lot faster:\n",
    "\n",
    "$q_{\\phi(x_0)}(z_n)=\\mathcal{N}(\\sqrt{\\bar a_n}x_0,(1-\\bar a_n)\\mathcal{I})$ where $\\bar a_n=\\prod_{i}^{n}a_i, a_i=1-\\beta_i$\n",
    "\n",
    "Using the reparameterization trick (see VAE), we can get any $z_n$ directly from $x_0$: $z_n=\\sqrt{\\bar a_n}x_0+\\sqrt{(1-\\bar a_n)}\\epsilon$ where $\\epsilon\\sim\\mathcal{N}(0,\\mathcal{I})$\n",
    "\n",
    "<br />\n",
    "\n",
    "### Learning $\\theta$ using the ELBO (Backward Process)\n",
    "The latent variable model $p_\\theta(x_0)=\\int p_\\theta(x_0, z_{1:N})dz_1 ... dz_N$ is intractable and therefore, maximizing the likelihood to obtain $\\theta$, i.e. $\\underset{\\theta}{\\mathrm{max}}\\:\\sum_{x_0} \\log p_\\theta(x_0)$ is intractable.\n",
    "\n",
    "But instead of maximizing the actual likelihood, we can maximize the ELBO.\n",
    "\n",
    "In the VAE notebook we derived the ELBO to be: $\\mathcal{L} = \\mathbb{E}_{z \\sim q(z)}[\\log\\left (\\frac{p(x,z)}{q(z)}\\right)]$ (Check the VAE notebook for the derivation)\n",
    "\n",
    "For our case here, this is equal to (plug in the formulations for the distributions):\n",
    "\n",
    "$\\log p_\\theta(x_0)\\geq\\mathbb{E}_{q(z_{1:N})}[\\log p_\\theta(x_0,z_{1:N})-\\log q_{\\phi(x_0)}(z_{1:N})]=\\mathbb{E}_{q(z_{1:N})}[\\log p(z_N)+\\sum_{n>1}\\log p_\\theta(z_{n-1}|z_n)+\\log p_\\theta(x_0|z_1)-\\sum_{n>1}\\log q_{\\phi(x_0)}(z_n|z_{n-1})-\\log q_{\\phi(x_0)}(z_1)]$\n",
    "\n",
    "Now, let's match the parts for the KL Divergences\n",
    "\n",
    "$=\\mathbb{E}_{q(z_{1:N})}[\\log p(z_N)+\\sum_{n>1}\\log \\frac {p_\\theta(z_{n-1}|z_n)}{\\log q_{\\phi(x_0)}(z_n|z_{n-1})}+\\log \\frac {p_\\theta(x_0|z_1)}{\\log q_{\\phi(x_0)}(z_1)}]$\n",
    "\n",
    "Note that: $q_{\\phi(x_0)}(z_n|z_{n-1}) = \\frac {q_{\\phi(x_0)}(z_{n-1}|z_n)q_{\\phi(x_0)}(z_n)} {q_{\\phi(x_0)}(z_{n-1})}$. Therefore we can rewrite to:\n",
    "\n",
    "$=\\mathbb{E}_{q(z_{1:N})}[\\log p(z_N)+\\sum_{n>1}\\log \\frac {p_\\theta(z_{n-1}|z_n)}{q_{\\phi(x_0)}(z_{n-1}|z_n)} \\frac {q_{\\phi(x_0)}(z_{n-1})} {\\phi(x_0)(z_n)} +\\log \\frac {p_\\theta(x_0|z_1)}{\\log q_{\\phi(x_0)}(z_1)}]$\n",
    "\n",
    "$=\\mathbb{E}_{q(z_{1:N})}[\\log p(z_N)+\\sum_{n>1}\\log \\frac {p_\\theta(z_{n-1}|z_n)}{q_{\\phi(x_0)}(z_{n-1}|z_n)} +\\sum_{n>1}\\log \\frac {q_{\\phi(x_0)}(z_{n-1})} {\\phi(x_0)(z_n)} +\\log \\frac {p_\\theta(x_0|z_1)}{\\log q_{\\phi(x_0)}(z_1)}]$\n",
    "\n",
    "Note that: $\\sum_{n>1}\\log \\frac {q_{\\phi(x_0)}(z_{n-1})} {\\phi(x_0)(z_n)} = \\log \\frac {q_{\\phi(x_0)}(z_{1})} {\\phi(x_0)(z_2)} + \\log \\frac {q_{\\phi(x_0)}(z_{2})} {\\phi(x_0)(z_3)} + ... + \\log \\frac {q_{\\phi(x_0)}(z_{n-1})} {\\phi(x_0)(z_n)} = \\log \\frac {q_{\\phi(x_0)}(z_{1})*\\sout{q_{\\phi(x_0)}(z_{2})}*...*\\sout{q_{\\phi(x_0)}(z_{n-1})}} {\\sout{q{\\phi(x_0)}(z_2)}*\\sout{q{\\phi(x_0)}(z_3)}*...*q{\\phi(x_0)}(z_n)} = \\log q_{\\phi(x_0)}(z_1) - \\log q_{\\phi(x_0)}(z_n) $\n",
    "\n",
    "Thus, we get this expression in which we can cancel further terms:\n",
    "\n",
    "$=\\mathbb{E}_{q(z_{1:N})}[\\log p(z_N)+\\sum_{n>1}\\log \\frac {p_\\theta(z_{n-1}|z_n)}{q_{\\phi(x_0)}(z_{n-1}|z_n)} + \\log \\sout{q_{\\phi(x_0)}(z_1)} - \\log q_{\\phi(x_0)}(z_n)  +\\log \\frac {p_\\theta(x_0|z_1)}{\\sout{\\log q_{\\phi(x_0)}(z_1)}}]$\n",
    "\n",
    "$=\\mathbb{E}_{q(z_{1:N})}[\\log \\frac {p(z_N)}{q_{\\phi(x_0)}(z_n)}+\\sum_{n>1}\\log \\frac {p_\\theta(z_{n-1}|z_n)}{q_{\\phi(x_0)}(z_{n-1}|z_n)} + \\log p_\\theta(x_0|z_1)]$\n",
    "\n",
    "Lastly, we distribute the expected value and flip the first two fractions by pulling out a minus\n",
    "\n",
    "\n",
    "$=-\\mathbb{E}_{q(z_{1:N})}[\\log \\frac {q_{\\phi(x_0)}(z_n)}{p(z_N)}] - \\sum_{n>1} \\mathbb{E}_{q(z_{1:N})}[\\log \\frac {q_{\\phi(x_0)}(z_{n-1}|z_n)}{p_\\theta(z_{n-1}|z_n)}] + \\mathbb{E}_{q(z_{1:N})}[\\log p_\\theta(x_0|z_1)]$\n",
    "\n",
    "$=-\\text{KL}[q_{\\phi(x_0)}(z_n) || p(z_N)] - \\sum_{n>1}\\text{KL}[q_{\\phi(x_0)}(z_{n-1}|z_n) || p_\\theta(z_{n-1}|z_n)] + \\mathbb{E}_{q(z_{1:N})}[\\log p_\\theta(x_0|z_1)]$\n",
    "\n",
    "Note that $p(z_N)$ is known and we want to optimize for $\\theta$, so we can get rid of the first term, which finally gets us to this expression:\n",
    "\n",
    "$\\mathbf{=\\mathbb{E}_{z_1 \\sim q_{\\phi(x_0)}(z_1)} [\\log p_\\theta(x_0|z_1)] - \\sum_{n>1}\\text{KL}[q_{\\phi(x_0)}(z_{n-1}|z_n) || p_\\theta(z_{n-1}|z_n)]}$\n",
    "\n",
    "where $q_{\\phi(x_0)}(z_{n-1}|z_n)=\\mathcal{N}(\\widetilde{\\mu}(x_0,z_n,n),\\tilde{\\beta}_n \\mathcal{I})$ with\n",
    "\n",
    "$\\widetilde{\\mu}(x_0,z_n,n)=\\frac {\\sqrt{a_n}-(1-\\bar a_{n-1})}{1-\\bar a_n}z_n + (\\sqrt{\\bar a_{n-1}}\\beta_n)$ and $\\tilde{\\beta}_n = \\frac {1-\\bar a_{n-1}}{1-\\bar a_n}\\beta_n$\n",
    "\n",
    "Since all distributions are Gaussians, we can calculate the loss in closed-form.\n",
    "\n",
    "So, we optimize two things with this equation:\n",
    "1. With $\\mathbb{E}_{z_1 \\sim q_{\\phi(x_0)}(z_1)} [\\log p_\\theta(x_0|z_1)]$ optimize for the reconstruction of the image $x_0$ from the latent variable $z_1$.\n",
    "2. With $\\sum_{n>1}\\text{KL}[q_{\\phi(x_0)}(z_{n-1}|z_n) || p_\\theta(z_{n-1}|z_n)]$ we aim to make the transitions in p similar to those in q.\n",
    "\n",
    "<br />\n",
    "\n",
    "### Model Parameterization\n",
    "We defined $p_\\theta(z_{n-1}|z_n)=\\mathcal{N}(\\mu_\\theta(z_n,n),\\Sigma_\\theta(z_n,n))$. But how should we choose $\\mu_\\theta$ and $\\Sigma_\\theta$?\n",
    "\n",
    "As already mentioned, we fix $\\Sigma_\\theta$. For simplicity, we let $\\Sigma_\\theta:=\\tilde \\beta_n \\mathcal{I}$\n",
    "\n",
    "Additionally, to minimize $\\sum_{n>1}\\text{KL}[q_{\\phi(x_0)}(z_{n-1}|z_n) || p_\\theta(z_{n-1}|z_n)]$, it makes sense to define $\\mu_\\theta(z_n,n) := \\widetilde{\\mu}(x_0,z_n,n)$\n",
    "\n",
    "Notably, $\\widetilde{\\mu}$ requires $x_0$, which we don't have in the reverse process. However, we can estimate it:\n",
    "\n",
    "By reparameterization: $z_n=\\sqrt{\\bar a_n}x_0 + \\sqrt{(1-\\bar a_n)}\\epsilon$\n",
    "\n",
    "Rewriting this, we can estimate $x_0$ given $z_n$ by predicting the noise ($\\epsilon$):\n",
    "\n",
    "$x_0 \\approx f_\\theta(z_n,n)=\\frac{z_n - \\sqrt{(1-\\bar a_n)}\\epsilon_\\theta(z_n,n)}{\\sqrt{\\bar a_n}}$\n",
    "\n",
    "Note what this means: We only need to predict the noise!\n",
    "\n",
    "<br />\n",
    "\n",
    "### The Final Loss\n",
    "We have seen that in practice, we only need to predict the noise. For this reason, it is common to use a simpler loss:\n",
    "\n",
    "$\\mathbf{\\mathcal{L}=\\mathbb{E}_{n,x_0,\\epsilon}[||\\epsilon-\\epsilon_\\theta(z_n,n)||^2] = \\mathbb{E}_{n,x_0,\\epsilon}[||\\epsilon-\\epsilon_\\theta(\\sqrt{\\bar a_n}x_0 + \\sqrt{(1-\\bar a_n)}\\epsilon,n)||^2]}$\n",
    "\n",
    "\n",
    "### The Training Loop\n",
    "tbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
